{
    "algorithm": "bfs",
    "model": "llama2",
    "model_path" : "/local2/ataylor2/llama/llama-7B-hf",
    "data_path": "/local2/ataylor2/algorithmic_reasoning/bfs",
    "cache_dir": "/local2/ataylor2/algorithmic_reasoning/cache/bfs",
    "dataset_type": "llm_data",
    "hint_level": "CoT",
    "data_format": "json",
    "output_path": "CoT_output",
    "show_reasoning": false,
    "use_trl": false,
    "use_peft": true,
    "model_max_length": 4096,
    "response_template": [
        2277,
        29937,
        830,
        496,
        519,
        405,
        2631,
        29901
    ],
    "device": "cuda:4",
    "do_train": true,
    "do_eval":true
}